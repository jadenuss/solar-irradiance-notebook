{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from geopy.geocoders import Nominatim\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "place = input (\"Enter Address :\") \n",
    "print(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place = \"408 Brook Pine Trl, apex\"\n",
    "geolocator = Nominatim(user_agent=\"Test\")\n",
    "location = geolocator.geocode(place) \n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((location.latitude, location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Declare all variables as strings. Spaces must be replaced with '+', i.e., change 'John Smith' to 'John+Smith'.\n",
    "  # Define the lat, long of the location and the year\n",
    "  #lat, lon, year = 33.2164, -97.1292, 2018\n",
    "  lat, lon =  location.latitude, location.longitude\n",
    "  # You must request an NSRDB api key from the link above\n",
    "  api_key = '5qyFRrBVjEZIGuR0WEcihqCEcg4LV8DbErgE6rze'\n",
    "  # Set the attributes to extract (e.g., dhi, ghi, etc.), separated by commas.\n",
    "  attributes = 'ghi,dhi,dni,wind_speed,air_temperature,solar_zenith_angle'\n",
    "  #attributes = 'ghi'\n",
    "  # Choose year of data\n",
    "  year = '2018'\n",
    "  # Set leap year to true or false. True will return leap day data if present, false will not.\n",
    "  leap_year = 'false'\n",
    "  # Set time interval in minutes, i.e., '30' is half hour intervals. Valid intervals are 30 & 60.\n",
    "  interval = '60'\n",
    "  # Specify Coordinated Universal Time (UTC), 'true' will use UTC, 'false' will use the local time zone of the data.\n",
    "  # NOTE: In order to use the NSRDB data in SAM, you must specify UTC as 'false'. SAM requires the data to be in the\n",
    "  # local time zone.\n",
    "  utc = 'false'\n",
    "  # Your full name, use '+' instead of spaces.\n",
    "  your_name = 'Anthony+N'\n",
    "  # Your reason for using the NSRDB.\n",
    "  reason_for_use = 'testing'\n",
    "  # Your affiliation\n",
    "  your_affiliation = 'ECU'\n",
    "  # Your email address\n",
    "  your_email = 'natalea20@students.ecu.edu'\n",
    "  # Please join our mailing list so we can keep you up-to-date on new developments.\n",
    "  mailing_list = 'false'\n",
    "\n",
    "  # Declare url string\n",
    "  url = 'https://developer.nrel.gov/api/solar/nsrdb_psm3_download.csv?wkt=POINT({lon}%20{lat})&names={year}&leap_day={leap}&interval={interval}&utc={utc}&full_name={name}&email={email}&affiliation={affiliation}&mailing_list={mailing_list}&reason={reason}&api_key={api}&attributes={attr}'.format(year=year, lat=lat, lon=lon, leap=leap_year, interval=interval, utc=utc, name=your_name, email=your_email, mailing_list=mailing_list, affiliation=your_affiliation, reason=reason_for_use, api=api_key, attr=attributes)\n",
    "  # Return just the first 2 lines to get metadata:\n",
    "  info = pd.read_csv(url, nrows=1)\n",
    "  # See metadata for specified properties, e.g., timezone and elevation\n",
    "  timezone, elevation = info['Local Time Zone'], info['Elevation']\n",
    "  \n",
    "  df = pd.read_csv(url, nrows=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View metadata\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, skiprows=2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the time index in the pandas dataframe:\n",
    "df = df.set_index(pd.date_range('1/1/{yr}'.format(yr=year), freq=interval+'Min', periods=525600/int(interval)))\n",
    "\n",
    "# take a look\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print column names\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the GHI Data for the Year 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(df.index, df['GHI'], alpha = 0.5, linewidth=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are many ups and downs - these correspond to the time of day. Naturally, GHI is far lower during the night. The beginning and end of this graph is the evening and the center is when the sun is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomedin = plt.subplot()\n",
    "zoomedin.margins(x=-0.498)\n",
    "zoomedin.plot(df.index, df['GHI'])\n",
    "zoomedin.set_title('Subset Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sample.csv\")\n",
    "# Connect to MongoDB\n",
    "client =  MongoClient(\"mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false\")\n",
    "db = client['Solar']\n",
    "collection = db['Local']\n",
    "df.reset_index(inplace=True)\n",
    "data_dict = df.to_dict(\"records\")\n",
    "# Insert collection\n",
    "collection.insert_many(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, Let's Run a Linear Regression Algorithm on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 3 Years of Data (2017-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,r2_score,max_error\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Address :536 Garrison Ct SW Concord NC 28025\n"
     ]
    }
   ],
   "source": [
    "place = input (\"Enter Address :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"Test\")\n",
    "location = geolocator.geocode(place)\n",
    "\n",
    "years = ['2017','2018','2019']\n",
    "df = []\n",
    "\n",
    "lat, lon =  location.latitude, location.longitude\n",
    "api_key = '5qyFRrBVjEZIGuR0WEcihqCEcg4LV8DbErgE6rze'\n",
    "attributes = 'ghi'\n",
    "leap_year = 'false'\n",
    "interval = '60'\n",
    "utc = 'false'\n",
    "your_name = 'Anthony+N'\n",
    "reason_for_use = 'testing'\n",
    "your_affiliation = 'ECU'\n",
    "your_email = 'natalea20@students.ecu.edu'\n",
    "mailing_list = 'false'\n",
    "for year in years:\n",
    "    url = 'https://developer.nrel.gov/api/solar/nsrdb_psm3_download.csv?wkt=POINT({lon}%20{lat})&names={year}&leap_day={leap}&interval={interval}&utc={utc}&full_name={name}&email={email}&affiliation={affiliation}&mailing_list={mailing_list}&reason={reason}&api_key={api}&attributes={attr}'.format(year=year, lat=lat, lon=lon, leap=leap_year, interval=interval, utc=utc, name=your_name, email=your_email, mailing_list=mailing_list, affiliation=your_affiliation, reason=reason_for_use, api=api_key, attr=attributes)\n",
    "    df.append(pd.read_csv(url, skiprows=2))\n",
    "\n",
    "noon_values = pd.DataFrame(columns=['Year','Month','Day','Hour','Minute','GHI'])\n",
    "# Append 2017 noon values\n",
    "noon_2017 = df[0].loc[df[0]['Hour'] == 12]\n",
    "noon_values = noon_values.append(noon_2017)\n",
    "# Append 2018 noon values\n",
    "noon_2018 = df[1].loc[df[1]['Hour'] == 12]\n",
    "noon_values = noon_values.append(noon_2018)\n",
    "# Append 2019 noon values\n",
    "noon_2019 = df[2].loc[df[2]['Hour'] == 12]\n",
    "noon_values = noon_values.append(noon_2019)\n",
    "\n",
    "# Reset the index\n",
    "noon_values = noon_values.reset_index()\n",
    "# Eliminate the repetitive columns\n",
    "# Leaves us with the datetime as the index and the GHI value at noon as the info\n",
    "del noon_values['index']\n",
    "del noon_values['Minute']\n",
    "del noon_values['Hour']\n",
    "noon_values['index1'] = noon_values.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and GHI values\n",
    "features = noon_values[['Year','Month','Day']]\n",
    "results = noon_values['GHI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, results, test_size=0.33)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "# Train the model on the scaled data\n",
    "model = LinearRegression()\n",
    "model.fit(X_scaled,y_train)\n",
    "ghi_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01652047, -1.00884966, -0.28384944],\n",
       "       [ 0.01652047,  1.3101995 , -0.17113999],\n",
       "       [ 0.01652047, -0.71896852, -0.50926833],\n",
       "       ...,\n",
       "       [-1.19442962, -0.13920623,  0.73053557],\n",
       "       [ 1.22747055,  0.44055606, -0.28384944],\n",
       "       [ 1.22747055, -0.42908737,  1.74492058]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  1119820948.3648024 \n",
      "R-Squared Error:  -14714.405602525712 \n",
      "Max Error:  33896.83463992829\n"
     ]
    }
   ],
   "source": [
    "lin_mse = mean_squared_error(y_test,ghi_prediction)\n",
    "lin_r2 = r2_score(y_test,ghi_prediction)\n",
    "lin_maxerror = max_error(y_test,ghi_prediction)\n",
    "print(\"Mean Squared Error: \",lin_mse,\"\\nR-Squared Error: \",lin_r2,\"\\nMax Error: \",lin_maxerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold, cross_val_predict\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from scipy.stats import uniform as sp_rand\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use 10-Fold Cross Validation to select the best model\n",
    "# Our K will be 10 for 10-fold validation\n",
    "k = 10\n",
    "# Linear Regression Cross Validation Scores & Predictions\n",
    "linear = LinearRegression()\n",
    "linear_cross_val_scores = cross_val_score(linear, X_train, y_train, cv=k)\n",
    "linear_cross_val_predict = cross_val_predict(linear, X_train, y_train, cv=k)\n",
    "print(\"Linear Regression Cross Val Scores: \", linear_cross_val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=k, random_state=None)\n",
    "ridge_best_alpha_average = 0\n",
    "lasso_best_alpha_average = 0\n",
    "# Split the day characteristic data along the indices chosen by the KFold object, and for each split...\n",
    "for train_index, test_index in kf.split(X_mod):\n",
    "    # Get our day characteristic training and test data\n",
    "    X_train, X_test = X_mod[train_index], X_mod[test_index]\n",
    "    # Get our total number of cyclists training and test data\n",
    "    y_train, y_test = y_mod[train_index], y_mod[test_index]\n",
    "    # Reshape the y_train data a bit\n",
    "    # y_train = y_train.reshape(-1, 1)\n",
    "    # X_train = X_train.reshape(-1, 1)\n",
    "    # X_test = X_test.reshape(-1, 1)\n",
    "    # y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    # TODO: Use RandomizedSearchCV to tune the alpha parameter for Ridge\n",
    "    alpha_grid = {\"alpha\": sp_rand()}\n",
    "    rr = Ridge()\n",
    "    ridge_rscv = RandomizedSearchCV(estimator=rr, param_distributions=alpha_grid, cv=2, n_iter=100)\n",
    "    ridge_rscv.fit(X_train, y_train)\n",
    "    # Record the best alpha parameter for ridge\n",
    "    ridge_best_alpha = ridge_rscv.best_estimator_.alpha\n",
    "    ridge_best_alpha_average += ridge_best_alpha\n",
    "\n",
    "    # TODO: Use RandomizedSearchCV to tune the alpha parameter for Lasso\n",
    "    lasso = Lasso(tol=0.01)\n",
    "    lasso_rscv = RandomizedSearchCV(estimator=lasso, param_distributions=alpha_grid, cv=2, n_iter=100)\n",
    "    lasso_rscv.fit(X_train, y_train)\n",
    "    # Record the best alpha parameter for lasso\n",
    "    lasso_best_alpha = lasso_rscv.best_estimator_.alpha\n",
    "    lasso_best_alpha_average += lasso_best_alpha\n",
    "\n",
    "ridge_best_alpha_average /= 10\n",
    "lasso_best_alpha_average /= 10\n",
    "\n",
    "print(\"Ridge Best Alpha: \", ridge_best_alpha_average)\n",
    "print(\"Lasso Best Alpha: \", lasso_best_alpha_average)\n",
    "\n",
    "# Lasso\n",
    "lasso = Lasso(alpha=lasso_best_alpha_average, tol=0.01)\n",
    "lasso_cross_val_scores = cross_val_score(lasso, X_mod, y_mod, cv=k)\n",
    "lasso_cross_val_predict = cross_val_predict(lasso, X_mod, y_mod, cv=k)\n",
    "print(\"Lasso Cross Val Scores: \", lasso_cross_val_scores)\n",
    "\n",
    "# Ridge\n",
    "ridge = Ridge(alpha=ridge_best_alpha_average)\n",
    "ridge_cross_val_scores = cross_val_score(ridge, X_mod, y_mod, cv=k)\n",
    "ridge_cross_val_predict = cross_val_predict(ridge, X_mod, y_mod, cv=k)\n",
    "print(\"Ridge Cross Val Scores: \", ridge_cross_val_scores)\n",
    "\n",
    "# Plot the linear regression, ridge, and lasso fit lines against the data\n",
    "plt.scatter(X_mod[:, 8], y_mod, label=\"Actual Data\", color=\"y\", alpha=.7)\n",
    "plt.plot(X_mod[:, 8], linear_cross_val_predict, label=\"Linear Regression\", color=\"b\", linewidth=1)\n",
    "# Plot both the ridge regression line and the test data (as points)\n",
    "plt.plot(X_mod[:, 8], ridge_cross_val_predict, label=\"Ridge Regression\", color=\"g\", linewidth=1)\n",
    "# Plot both the lasso regression line and the test data (as points)\n",
    "plt.plot(X_mod[:, 8], lasso_cross_val_predict, label=\"Lasso Regression\", color=\"r\", linewidth=1)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cyclists\")\n",
    "plt.xlabel(\"Hours of Daylight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean Squared Error: \")\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_mod, linear_cross_val_predict))\n",
    "print(\"Mean Squared Error: \",mean_squared_error(y_mod, ridge_cross_val_predict))\n",
    "print(\"Mean Squared Error: \",mean_squared_error(y_mod, lasso_cross_val_predict))\n",
    "\n",
    "print(\"R^2 Score: \")\n",
    "print(\"Linear Regression: \", r2_score(y_mod, linear_cross_val_predict, multioutput='uniform_average'))\n",
    "print(\"Linear Regression: \", r2_score(y_mod, linear_cross_val_predict, multioutput='uniform_average'))\n",
    "print(\"Linear Regression: \", r2_score(y_mod, linear_cross_val_predict, multioutput='uniform_average'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
